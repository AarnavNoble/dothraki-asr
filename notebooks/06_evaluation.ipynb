{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06 — Pipeline Evaluation\n",
    "\n",
    "Comprehensive evaluation of the full Dothraki ASR pipeline:\n",
    "Audio → Whisper → Phonemize → Match → Translate.\n",
    "\n",
    "## Contents\n",
    "1. [Overall Metrics](#1-overall-metrics) — Pipeline success rates\n",
    "2. [Translation Quality](#2-translation-quality) — How good are the translations?\n",
    "3. [Error Breakdown](#3-error-breakdown) — Where does the pipeline fail?\n",
    "4. [Model Comparison](#4-model-comparison) — Tiny vs Small across all metrics\n",
    "5. [Conclusions](#5-conclusions) — What we learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / 'data' / 'results'\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "eval_tiny = json.loads((RESULTS_DIR / 'batch_eval_tiny.json').read_text())\n",
    "eval_small = json.loads((RESULTS_DIR / 'batch_eval_small.json').read_text())\n",
    "\n",
    "print(f'Evaluations loaded:')\n",
    "print(f'  whisper-tiny:  {eval_tiny[\"num_clips\"]} clips')\n",
    "print(f'  whisper-small: {eval_small[\"num_clips\"]} clips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Overall Metrics\n",
    "\n",
    "High-level pipeline performance across all evaluation clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive metrics table\n",
    "def compute_extended_stats(data):\n",
    "    s = data['stats']\n",
    "    results = data['results']\n",
    "    valid = [r for r in results if 'error' not in r]\n",
    "    nonempty = [r for r in valid if r.get('whisper_text', '').strip()]\n",
    "    \n",
    "    # Translation stats\n",
    "    has_translation = [r for r in valid if r.get('pipeline_translation', '').strip()]\n",
    "    \n",
    "    # Word count stats\n",
    "    whisper_word_counts = [len(r['whisper_text'].split()) for r in nonempty]\n",
    "    gt_word_counts = [len(r['gt_dothraki'].split()) for r in nonempty]\n",
    "    \n",
    "    return {\n",
    "        'Total clips': s['total_clips'],\n",
    "        'Successful': s['successful'],\n",
    "        'Errors': s['errors'],\n",
    "        'Empty rate': f\"{s['empty_rate']*100:.1f}%\",\n",
    "        'Non-empty': s['nonempty_transcriptions'],\n",
    "        'Has translation': len(has_translation),\n",
    "        'Translation rate': f\"{len(has_translation)/s['successful']*100:.1f}%\",\n",
    "        'Avg Whisper words': f\"{np.mean(whisper_word_counts):.1f}\" if whisper_word_counts else 'N/A',\n",
    "        'Avg GT words': f\"{np.mean(gt_word_counts):.1f}\" if gt_word_counts else 'N/A',\n",
    "        'Top detected lang': max(s['language_distribution'], key=s['language_distribution'].get),\n",
    "    }\n",
    "\n",
    "tiny_stats = compute_extended_stats(eval_tiny)\n",
    "small_stats = compute_extended_stats(eval_small)\n",
    "\n",
    "print(f'{\"Metric\":25s} {\"whisper-tiny\":>15s} {\"whisper-small\":>15s}')\n",
    "print('=' * 58)\n",
    "for key in tiny_stats:\n",
    "    print(f'{key:25s} {str(tiny_stats[key]):>15s} {str(small_stats[key]):>15s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual pipeline funnel: how many clips survive each stage?\n",
    "stages = ['Input Clips', 'Whisper Success', 'Non-Empty Output', 'Has Translation']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, data, title in [(ax1, eval_tiny, 'whisper-tiny'), (ax2, eval_small, 'whisper-small')]:\n",
    "    s = data['stats']\n",
    "    counts = [\n",
    "        s['total_clips'],\n",
    "        s['successful'],\n",
    "        s['nonempty_transcriptions'],\n",
    "        s['translation_coverage'],\n",
    "    ]\n",
    "    \n",
    "    colors = ['#4ecdc4', '#45b7aa', '#3da190', '#ffd93d']\n",
    "    bars = ax.barh(stages[::-1], counts[::-1], color=colors[::-1], edgecolor='#1a1a2e')\n",
    "    \n",
    "    for bar, count in zip(bars, counts[::-1]):\n",
    "        pct = count / s['total_clips'] * 100\n",
    "        ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2,\n",
    "                f'{count} ({pct:.0f}%)', va='center', fontsize=11)\n",
    "    \n",
    "    ax.set_title(f'{title} — Pipeline Funnel', fontsize=13)\n",
    "    ax.set_xlim(0, s['total_clips'] * 1.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Translation Quality\n",
    "\n",
    "Comparing pipeline translations against ground truth English translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show translation examples: GT English vs Pipeline Translation\n",
    "results = eval_small['results']\n",
    "has_both = [r for r in results \n",
    "            if 'error' not in r \n",
    "            and r.get('pipeline_translation', '').strip()\n",
    "            and r.get('gt_english', '').strip()]\n",
    "\n",
    "print(f'Clips with both GT and pipeline translations: {len(has_both)}\\n')\n",
    "print(f'{\"Ground Truth English\":40s} | {\"Pipeline Translation\"}')\n",
    "print('=' * 90)\n",
    "\n",
    "for r in has_both[:15]:\n",
    "    gt = r['gt_english'][:38]\n",
    "    pt = r['pipeline_translation'][:45]\n",
    "    print(f'{gt:40s} | {pt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word overlap analysis: how many GT English words appear in pipeline translation?\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "overlaps = []\n",
    "for r in has_both:\n",
    "    gt_words = tokenize(r['gt_english'])\n",
    "    pt_words = tokenize(r['pipeline_translation'])\n",
    "    if gt_words:\n",
    "        overlap = len(gt_words & pt_words) / len(gt_words)\n",
    "        overlaps.append(overlap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(overlaps, bins=20, color='#4ecdc4', edgecolor='#1a1a2e', alpha=0.8)\n",
    "ax.set_xlabel('Word Overlap Ratio (GT ∩ Pipeline / |GT|)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Translation Word Overlap with Ground Truth')\n",
    "ax.axvline(np.mean(overlaps), color='#ff6b6b', linestyle='--',\n",
    "           label=f'Mean: {np.mean(overlaps):.3f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean word overlap: {np.mean(overlaps):.3f}')\n",
    "print(f'Clips with >0 overlap: {sum(1 for o in overlaps if o > 0)}/{len(overlaps)}')\n",
    "print(f'Clips with >50% overlap: {sum(1 for o in overlaps if o > 0.5)}/{len(overlaps)}')\n",
    "print(f'\\nNote: Low overlap is expected — the pipeline translates word-by-word via')\n",
    "print(f'dictionary lookup, so grammatical structure and idioms are lost.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Error Breakdown\n",
    "\n",
    "Categorizing failure modes in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize errors for whisper-small\n",
    "s = eval_small['stats']\n",
    "results = eval_small['results']\n",
    "\n",
    "categories = {\n",
    "    'Whisper empty': s['empty_transcriptions'],\n",
    "    'Whisper errors': s['errors'],\n",
    "    'No translation': s['nonempty_transcriptions'] - s['translation_coverage'],\n",
    "    'Has translation': s['translation_coverage'],\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart of outcome categories\n",
    "colors_cat = ['#ff6b6b', '#ff4757', '#ffd93d', '#4ecdc4']\n",
    "ax1.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%',\n",
    "        colors=colors_cat, textprops={'fontsize': 11})\n",
    "ax1.set_title('Pipeline Outcome Distribution (whisper-small)')\n",
    "\n",
    "# Whisper output length for empty vs non-empty GT matches\n",
    "gt_short = [r for r in results if 'error' not in r and len(r['gt_dothraki']) < 20]\n",
    "gt_long = [r for r in results if 'error' not in r and len(r['gt_dothraki']) >= 20]\n",
    "\n",
    "short_empty = sum(1 for r in gt_short if not r.get('whisper_text', '').strip())\n",
    "long_empty = sum(1 for r in gt_long if not r.get('whisper_text', '').strip())\n",
    "\n",
    "x = ['Short GT (<20 chars)', 'Long GT (≥20 chars)']\n",
    "empty_rates = [\n",
    "    short_empty / len(gt_short) * 100 if gt_short else 0,\n",
    "    long_empty / len(gt_long) * 100 if gt_long else 0,\n",
    "]\n",
    "bars = ax2.bar(x, empty_rates, color=['#ff6b6b', '#4ecdc4'], edgecolor='#1a1a2e', alpha=0.8)\n",
    "for bar, rate in zip(bars, empty_rates):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=12)\n",
    "ax2.set_ylabel('Empty Transcription Rate (%)')\n",
    "ax2.set_title('Empty Rate by Ground Truth Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Short clips (<20 chars): {len(gt_short)}, empty rate: {short_empty/len(gt_short)*100:.1f}%')\n",
    "print(f'Long clips (≥20 chars):  {len(gt_long)}, empty rate: {long_empty/len(gt_long)*100:.1f}%')\n",
    "print(f'\\nShorter clips are harder — less audio context for Whisper to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Comparison\n",
    "\n",
    "Head-to-head comparison of whisper-tiny vs whisper-small across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar-style comparison (using grouped bar chart)\n",
    "metrics_compare = {\n",
    "    'Non-Empty\\nRate (%)': [],\n",
    "    'Translation\\nCoverage (%)': [],\n",
    "    'Top Lang\\nConfidence (%)': [],\n",
    "    'Avg Output\\nLength': [],\n",
    "    'Unique\\nLanguages': [],\n",
    "}\n",
    "\n",
    "for data in [eval_tiny, eval_small]:\n",
    "    s = data['stats']\n",
    "    metrics_compare['Non-Empty\\nRate (%)'].append((1 - s['empty_rate']) * 100)\n",
    "    metrics_compare['Translation\\nCoverage (%)'].append(s['translation_coverage_rate'] * 100)\n",
    "    top_lang_pct = max(s['language_distribution'].values()) / s['successful'] * 100\n",
    "    metrics_compare['Top Lang\\nConfidence (%)'].append(top_lang_pct)\n",
    "    metrics_compare['Avg Output\\nLength'].append(s['avg_whisper_output_length'])\n",
    "    metrics_compare['Unique\\nLanguages'].append(len(s['language_distribution']))\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "colors = ['#4ecdc4', '#ff6b6b']\n",
    "model_names = ['tiny', 'small']\n",
    "\n",
    "for ax, (metric, values) in zip(axes, metrics_compare.items()):\n",
    "    bars = ax.bar(model_names, values, color=colors, edgecolor='#1a1a2e', width=0.5)\n",
    "    ax.set_title(metric, fontsize=11)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{val:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "fig.suptitle('Whisper Model Size Comparison — All Metrics', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-clip agreement: do both models produce the same language detection?\n",
    "tiny_results = {r['id']: r for r in eval_tiny['results'] if 'error' not in r}\n",
    "small_results = {r['id']: r for r in eval_small['results'] if 'error' not in r}\n",
    "\n",
    "common_ids = set(tiny_results.keys()) & set(small_results.keys())\n",
    "agree = sum(1 for cid in common_ids\n",
    "            if tiny_results[cid]['whisper_lang'] == small_results[cid]['whisper_lang'])\n",
    "\n",
    "print(f'Language detection agreement (tiny vs small):')\n",
    "print(f'  Common clips: {len(common_ids)}')\n",
    "print(f'  Same language: {agree} ({agree/len(common_ids)*100:.1f}%)')\n",
    "print(f'  Different: {len(common_ids)-agree} ({(len(common_ids)-agree)/len(common_ids)*100:.1f}%)')\n",
    "\n",
    "# Show disagreements\n",
    "disagreements = [(cid, tiny_results[cid]['whisper_lang'], small_results[cid]['whisper_lang'])\n",
    "                 for cid in sorted(common_ids)\n",
    "                 if tiny_results[cid]['whisper_lang'] != small_results[cid]['whisper_lang']]\n",
    "\n",
    "if disagreements:\n",
    "    print(f'\\nSample disagreements:')\n",
    "    for cid, t_lang, s_lang in disagreements[:10]:\n",
    "        print(f'  {cid}: tiny={t_lang}, small={s_lang}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Conclusions\n",
    "\n",
    "### What Works\n",
    "- **Pipeline execution**: End-to-end processing completes on 100% of clips without crashes\n",
    "- **Language detection**: Whisper consistently identifies *some* language, giving the phonemizer a starting point\n",
    "- **Phoneme extraction**: gruut/espeak-ng successfully convert Whisper text to IPA\n",
    "- **Translation coverage**: ~75-78% of clips produce some translation output\n",
    "\n",
    "### What Doesn't Work (Yet)\n",
    "- **Semantic accuracy**: Translations are phonetically plausible but semantically poor\n",
    "- **Empty transcriptions**: ~22-25% of clips produce no output (Whisper gives up)\n",
    "- **Word-by-word matching**: Loses all grammatical structure and context\n",
    "- **Uniform edit distance**: Treating /p/→/b/ the same as /p/→/z/ loses articulatory similarity\n",
    "\n",
    "### Key Insights\n",
    "1. Zero-shot ASR on a truly unseen language is a **degenerate case** — but an informative one\n",
    "2. Whisper's language detector reveals **phonological similarity** between Dothraki and real languages\n",
    "3. The pipeline demonstrates the full **ASR → NLP → Translation** stack even when accuracy is low\n",
    "4. Larger Whisper models are more **confident** but not necessarily more **accurate** for unseen languages\n",
    "\n",
    "### Future Directions\n",
    "- Weighted edit distance (articulatory features)\n",
    "- N-gram phoneme matching for longer sequences\n",
    "- Fine-tuning on synthetic Dothraki audio\n",
    "- Beam search over multiple Whisper hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print('='*60)\n",
    "print('PIPELINE EVALUATION SUMMARY')\n",
    "print('='*60)\n",
    "for model_name, data in [('tiny', eval_tiny), ('small', eval_small)]:\n",
    "    s = data['stats']\n",
    "    print(f'\\nwhisper-{model_name}:')\n",
    "    print(f'  Clips evaluated:     {s[\"total_clips\"]}')\n",
    "    print(f'  Non-empty rate:      {(1-s[\"empty_rate\"])*100:.1f}%')\n",
    "    print(f'  Translation rate:    {s[\"translation_coverage_rate\"]*100:.1f}%')\n",
    "    print(f'  Primary language:    {max(s[\"language_distribution\"], key=s[\"language_distribution\"].get)} '\n",
    "          f'({max(s[\"language_distribution\"].values())/s[\"successful\"]*100:.0f}%)')\n",
    "    print(f'  Languages detected:  {len(s[\"language_distribution\"])}')\n",
    "print('\\n' + '='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}