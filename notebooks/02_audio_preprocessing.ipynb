{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 — Audio Preprocessing\n",
    "\n",
    "This notebook walks through the audio preprocessing pipeline:\n",
    "vocal isolation with Demucs, normalization, and synthetic audio generation.\n",
    "\n",
    "## Contents\n",
    "1. [Synthetic Audio Overview](#1-synthetic-audio) — espeak-ng generated clips\n",
    "2. [Audio Quality Analysis](#2-audio-quality) — Waveforms, spectrograms\n",
    "3. [Real Audio Processing](#3-real-audio) — Demucs vocal isolation before/after\n",
    "4. [Comparison](#4-comparison) — Synthetic vs real audio characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "SYNTHETIC_DIR = PROJECT_ROOT / 'data' / 'synthetic'\n",
    "RAW_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "manifest = json.loads((SYNTHETIC_DIR / 'manifest.json').read_text())\n",
    "print(f'Loaded manifest: {len(manifest)} synthetic clips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Synthetic Audio\n",
    "\n",
    "All 1,712 dialogue entries were synthesized from IPA transcriptions using espeak-ng.\n",
    "Each clip is a single Dothraki line, rendered as 16kHz mono WAV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalog synthetic audio files and their properties\n",
    "import soundfile as sf\n",
    "\n",
    "durations = []\n",
    "file_sizes = []\n",
    "sample_rates = []\n",
    "\n",
    "for entry in manifest:\n",
    "    wav_path = SYNTHETIC_DIR / entry['audio_file']\n",
    "    if wav_path.exists():\n",
    "        info = sf.info(str(wav_path))\n",
    "        durations.append(info.duration)\n",
    "        file_sizes.append(wav_path.stat().st_size / 1024)\n",
    "        sample_rates.append(info.samplerate)\n",
    "\n",
    "print(f'Clips analyzed: {len(durations)}')\n",
    "print(f'Sample rate: {set(sample_rates)}')\n",
    "print(f'Duration: min={min(durations):.2f}s, max={max(durations):.2f}s, mean={np.mean(durations):.2f}s')\n",
    "print(f'Total audio: {sum(durations)/60:.1f} minutes')\n",
    "print(f'Total size: {sum(file_sizes)/1024:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.hist(durations, bins=50, color='#4ecdc4', edgecolor='#1a1a2e', alpha=0.8)\n",
    "ax1.set_xlabel('Duration (seconds)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Synthetic Clip Duration Distribution')\n",
    "ax1.axvline(np.mean(durations), color='#ff6b6b', linestyle='--',\n",
    "            label=f'Mean: {np.mean(durations):.2f}s')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(file_sizes, bins=50, color='#ff6b6b', edgecolor='#1a1a2e', alpha=0.8)\n",
    "ax2.set_xlabel('File Size (KB)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Synthetic Clip File Size Distribution')\n",
    "ax2.axvline(np.mean(file_sizes), color='#4ecdc4', linestyle='--',\n",
    "            label=f'Mean: {np.mean(file_sizes):.0f}KB')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample synthetic waveform and spectrogram\n",
    "sample_entry = manifest[4]  # \"Khal vezhvén! M'athchomaroón!\"\n",
    "sample_path = SYNTHETIC_DIR / sample_entry['audio_file']\n",
    "\n",
    "data, sr = sf.read(str(sample_path))\n",
    "time_axis = np.arange(len(data)) / sr\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Waveform\n",
    "ax1.plot(time_axis, data, color='#4ecdc4', alpha=0.8, linewidth=0.5)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_title(f'Waveform: \"{sample_entry[\"dothraki\"]}\"')\n",
    "ax1.set_xlim(0, time_axis[-1])\n",
    "\n",
    "# Spectrogram\n",
    "ax2.specgram(data, NFFT=512, Fs=sr, noverlap=256, cmap='magma')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Frequency (Hz)')\n",
    "ax2.set_title('Spectrogram')\n",
    "ax2.set_ylim(0, 8000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Ground truth: {sample_entry[\"dothraki\"]}')\n",
    "print(f'IPA: {sample_entry[\"ipa\"]}')\n",
    "print(f'English: {sample_entry[\"english\"]}')\n",
    "print(f'Duration: {len(data)/sr:.2f}s, samples: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Audio Quality Analysis\n",
    "\n",
    "Comparing spectral characteristics across multiple clips to understand\n",
    "the consistency and quality of synthesized Dothraki speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare waveforms of 6 different clips side by side\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 10))\n",
    "sample_indices = [0, 4, 10, 25, 50, 100]\n",
    "\n",
    "for ax, idx in zip(axes.flat, sample_indices):\n",
    "    if idx >= len(manifest):\n",
    "        continue\n",
    "    entry = manifest[idx]\n",
    "    wav_path = SYNTHETIC_DIR / entry['audio_file']\n",
    "    if wav_path.exists():\n",
    "        d, sr = sf.read(str(wav_path))\n",
    "        t = np.arange(len(d)) / sr\n",
    "        ax.plot(t, d, color='#4ecdc4', alpha=0.8, linewidth=0.5)\n",
    "        ax.set_title(f'd{idx:04d}: \"{entry[\"dothraki\"][:40]}...\"' if len(entry['dothraki']) > 40\n",
    "                     else f'd{idx:04d}: \"{entry[\"dothraki\"]}\"', fontsize=10)\n",
    "        ax.set_xlim(0, t[-1])\n",
    "        ax.set_ylim(-1, 1)\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel('Time (s)')\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel('Amplitude')\n",
    "\n",
    "fig.suptitle('Sample Synthetic Waveforms', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration vs ground truth text length (characters)\n",
    "gt_lens = []\n",
    "clip_durations = []\n",
    "for entry in manifest:\n",
    "    wav_path = SYNTHETIC_DIR / entry['audio_file']\n",
    "    if wav_path.exists():\n",
    "        info = sf.info(str(wav_path))\n",
    "        gt_lens.append(len(entry['dothraki']))\n",
    "        clip_durations.append(info.duration)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(gt_lens, clip_durations, alpha=0.3, c='#4ecdc4', s=20, edgecolors='none')\n",
    "ax.set_xlabel('Ground Truth Text Length (characters)')\n",
    "ax.set_ylabel('Audio Duration (seconds)')\n",
    "ax.set_title('Text Length vs Synthesized Audio Duration')\n",
    "\n",
    "# Fit line\n",
    "z = np.polyfit(gt_lens, clip_durations, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(min(gt_lens), max(gt_lens), 100)\n",
    "ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'Linear fit (r={np.corrcoef(gt_lens, clip_durations)[0,1]:.3f})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Speaking rate: ~{np.mean(np.array(gt_lens)/np.array(clip_durations)):.1f} chars/sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Real Audio Processing\n",
    "\n",
    "Real Dothraki clips from Game of Thrones contain music, sound effects,\n",
    "and crowd noise. We use Demucs (htdemucs) to isolate vocal tracks.\n",
    "\n",
    "**Pipeline:** Raw clip → Demucs vocal separation → 16kHz mono WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available real and processed audio\n",
    "print('Raw audio clips:')\n",
    "for wav in sorted(RAW_DIR.glob('*.wav')):\n",
    "    info = sf.info(str(wav))\n",
    "    print(f'  {wav.name}: {info.duration:.1f}s, {info.samplerate}Hz, {info.channels}ch')\n",
    "\n",
    "print(f'\\nProcessed (Demucs isolated) clips:')\n",
    "for wav in sorted(PROCESSED_DIR.glob('*.wav')):\n",
    "    info = sf.info(str(wav))\n",
    "    print(f'  {wav.name}: {info.duration:.1f}s, {info.samplerate}Hz, {info.channels}ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs isolated vocals for the Drogo speech\n",
    "raw_path = RAW_DIR / 'drogo_rhaego_speech.wav'\n",
    "clean_path = PROCESSED_DIR / 'drogo_rhaego_speech_vocals.wav'\n",
    "\n",
    "if raw_path.exists() and clean_path.exists():\n",
    "    raw_data, raw_sr = sf.read(str(raw_path))\n",
    "    clean_data, clean_sr = sf.read(str(clean_path))\n",
    "\n",
    "    # Take just the first 30 seconds for visualization\n",
    "    raw_30s = raw_data[:raw_sr * 30] if len(raw_data) > raw_sr * 30 else raw_data\n",
    "    clean_30s = clean_data[:clean_sr * 30] if len(clean_data) > clean_sr * 30 else clean_data\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Raw waveform\n",
    "    raw_t = np.arange(len(raw_30s)) / raw_sr\n",
    "    if raw_30s.ndim > 1:\n",
    "        raw_mono = raw_30s.mean(axis=1)\n",
    "    else:\n",
    "        raw_mono = raw_30s\n",
    "    axes[0, 0].plot(raw_t, raw_mono, color='#ff6b6b', alpha=0.7, linewidth=0.3)\n",
    "    axes[0, 0].set_title('Raw Audio — Waveform (first 30s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "    # Raw spectrogram\n",
    "    axes[0, 1].specgram(raw_mono, NFFT=1024, Fs=raw_sr, noverlap=512, cmap='magma')\n",
    "    axes[0, 1].set_title('Raw Audio — Spectrogram')\n",
    "    axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "    axes[0, 1].set_ylim(0, 8000)\n",
    "\n",
    "    # Clean waveform\n",
    "    clean_t = np.arange(len(clean_30s)) / clean_sr\n",
    "    axes[1, 0].plot(clean_t, clean_30s, color='#4ecdc4', alpha=0.7, linewidth=0.3)\n",
    "    axes[1, 0].set_title('Isolated Vocals — Waveform (first 30s)')\n",
    "    axes[1, 0].set_ylabel('Amplitude')\n",
    "    axes[1, 0].set_xlabel('Time (s)')\n",
    "\n",
    "    # Clean spectrogram\n",
    "    axes[1, 1].specgram(clean_30s, NFFT=1024, Fs=clean_sr, noverlap=512, cmap='magma')\n",
    "    axes[1, 1].set_title('Isolated Vocals — Spectrogram')\n",
    "    axes[1, 1].set_ylabel('Frequency (Hz)')\n",
    "    axes[1, 1].set_xlabel('Time (s)')\n",
    "    axes[1, 1].set_ylim(0, 8000)\n",
    "\n",
    "    fig.suptitle('Demucs Vocal Isolation: Before vs After', fontsize=15, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Raw: {raw_data.shape}, {raw_sr}Hz')\n",
    "    print(f'Clean: {clean_data.shape}, {clean_sr}Hz')\n",
    "    print(f'RMS reduction: {np.sqrt(np.mean(raw_mono**2)):.4f} → {np.sqrt(np.mean(clean_30s**2)):.4f}')\n",
    "else:\n",
    "    print('Raw or processed files not found. Run the pipeline on real clips first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparison: Synthetic vs Real\n",
    "\n",
    "Comparing the spectral characteristics of synthesized Dothraki (espeak-ng)\n",
    "versus real actor performances from Game of Thrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side spectrograms: synthetic vs real (isolated vocals)\n",
    "synth_path = SYNTHETIC_DIR / manifest[4]['audio_file']\n",
    "\n",
    "# Find a processed real clip\n",
    "real_vocals = sorted(PROCESSED_DIR.glob('*_vocals.wav'))\n",
    "real_path = real_vocals[0] if real_vocals else None\n",
    "\n",
    "if synth_path.exists() and real_path and real_path.exists():\n",
    "    synth_data, synth_sr = sf.read(str(synth_path))\n",
    "    real_data, real_sr = sf.read(str(real_path))\n",
    "\n",
    "    # Take first 5 seconds of real audio\n",
    "    real_5s = real_data[:real_sr * 5]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    ax1.specgram(synth_data, NFFT=512, Fs=synth_sr, noverlap=256, cmap='magma')\n",
    "    ax1.set_title(f'Synthetic (espeak-ng)\\n\"{manifest[4][\"dothraki\"][:50]}\"')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Frequency (Hz)')\n",
    "    ax1.set_ylim(0, 8000)\n",
    "\n",
    "    ax2.specgram(real_5s, NFFT=512, Fs=real_sr, noverlap=256, cmap='magma')\n",
    "    ax2.set_title(f'Real Audio (GoT, isolated vocals)\\n\"{real_path.stem}\"')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "    ax2.set_ylim(0, 8000)\n",
    "\n",
    "    fig.suptitle('Synthetic vs Real Dothraki Audio', fontsize=15, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Key differences:')\n",
    "    print('  Synthetic: clean, uniform energy, no background noise')\n",
    "    print('  Real: dynamic range, emotional prosody, residual reverb/noise')\n",
    "    print('  Synthetic lacks the natural pitch variation of actor performance')\n",
    "else:\n",
    "    print('Need both synthetic and real audio files for comparison.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "print('='*60)\n",
    "print('AUDIO PREPROCESSING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Synthetic clips:      {len(durations)}')\n",
    "print(f'Total synth duration: {sum(durations)/60:.1f} minutes')\n",
    "print(f'Avg clip duration:    {np.mean(durations):.2f}s')\n",
    "print(f'Sample rate:          16kHz (mono)')\n",
    "print(f'Real clips (raw):     {len(list(RAW_DIR.glob(\"*.wav\")))}')\n",
    "print(f'Real clips (processed): {len(list(PROCESSED_DIR.glob(\"*.wav\")))}')\n",
    "print(f'Separation model:     Demucs htdemucs')\n",
    "print(f'Synthesis engine:     espeak-ng (IPA mode)')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}