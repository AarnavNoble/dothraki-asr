{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Zero-Shot Whisper on Dothraki\n",
    "\n",
    "This notebook explores how OpenAI's Whisper model behaves when given speech in\n",
    "a language it has **never seen**: Dothraki, a constructed language from Game of Thrones.\n",
    "\n",
    "Whisper was trained on 680,000 hours of multilingual audio covering 99 languages.\n",
    "Dothraki is not one of them. What happens when we feed it Dothraki speech?\n",
    "\n",
    "## Contents\n",
    "1. [Language Detection](#1-language-detection) — What language does Whisper *think* it's hearing?\n",
    "2. [Model Size Comparison](#2-model-size-comparison) — How do tiny vs small models differ?\n",
    "3. [Transcription Analysis](#3-transcription-analysis) — What text does Whisper output?\n",
    "4. [Real vs Synthetic Audio](#4-real-vs-synthetic) — How does the source affect results?\n",
    "5. [Key Findings](#5-key-findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "RESULTS_DIR = PROJECT_ROOT / 'data' / 'results'\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Load evaluation results\n",
    "eval_tiny = json.loads((RESULTS_DIR / 'batch_eval_tiny.json').read_text())\n",
    "eval_small = json.loads((RESULTS_DIR / 'batch_eval_small.json').read_text())\n",
    "\n",
    "print(f\"Loaded: whisper-tiny ({eval_tiny['num_clips']} clips), whisper-small ({eval_small['num_clips']} clips)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Language Detection\n",
    "\n",
    "Whisper's first step is language identification. Since Dothraki isn't in its training data,\n",
    "it must pick the closest-sounding language it knows. This tells us which real languages\n",
    "Dothraki phonetically resembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, data, title in [\n",
    "    (ax1, eval_tiny, 'whisper-tiny'),\n",
    "    (ax2, eval_small, 'whisper-small'),\n",
    "]:\n",
    "    lang_dist = data['stats']['language_distribution']\n",
    "    # Take top 8, group rest as 'other'\n",
    "    top_langs = dict(list(lang_dist.items())[:8])\n",
    "    other = sum(v for k, v in lang_dist.items() if k not in top_langs)\n",
    "    if other > 0:\n",
    "        top_langs['other'] = other\n",
    "\n",
    "    labels = list(top_langs.keys())\n",
    "    values = list(top_langs.values())\n",
    "    total = sum(values)\n",
    "\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(labels)))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        values, labels=labels, autopct=lambda p: f'{p:.0f}%' if p > 3 else '',\n",
    "        colors=colors, textprops={'fontsize': 11}\n",
    "    )\n",
    "    ax.set_title(f'{title}\\n({total} clips)', fontsize=14)\n",
    "\n",
    "fig.suptitle('What Language Does Whisper Think Dothraki Is?', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison table\n",
    "all_langs = set()\n",
    "for data in [eval_tiny, eval_small]:\n",
    "    all_langs.update(data['stats']['language_distribution'].keys())\n",
    "\n",
    "print(f\"{'Language':<10} {'tiny':>8} {'small':>8}\")\n",
    "print('-' * 30)\n",
    "for lang in sorted(all_langs):\n",
    "    tiny_count = eval_tiny['stats']['language_distribution'].get(lang, 0)\n",
    "    small_count = eval_small['stats']['language_distribution'].get(lang, 0)\n",
    "    if tiny_count + small_count > 1:\n",
    "        print(f\"{lang:<10} {tiny_count:>8} {small_count:>8}\")\n",
    "\n",
    "print(f\"\\nKey insight: Whisper-tiny shows more language confusion (detects Thai, Korean, Japanese)\")\n",
    "print(f\"while whisper-small converges more strongly on English (~86%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Size Comparison\n",
    "\n",
    "Comparing how different Whisper model sizes handle zero-shot Dothraki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "models = ['tiny', 'small']\n",
    "evals = [eval_tiny, eval_small]\n",
    "\n",
    "metrics = {\n",
    "    'Empty Rate (%)': [],\n",
    "    'Avg Output Length': [],\n",
    "    'Translation Coverage (%)': [],\n",
    "    'Top Language %': [],\n",
    "}\n",
    "\n",
    "for data in evals:\n",
    "    s = data['stats']\n",
    "    metrics['Empty Rate (%)'].append(s['empty_rate'] * 100)\n",
    "    metrics['Avg Output Length'].append(s['avg_whisper_output_length'])\n",
    "    metrics['Translation Coverage (%)'].append(s['translation_coverage_rate'] * 100)\n",
    "    top_lang_pct = max(s['language_distribution'].values()) / s['successful'] * 100\n",
    "    metrics['Top Language %'].append(top_lang_pct)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "colors = ['#4ecdc4', '#ff6b6b']\n",
    "\n",
    "for ax, (metric_name, values) in zip(axes, metrics.items()):\n",
    "    bars = ax.bar(models, values, color=colors, edgecolor='#1a1a2e', width=0.5)\n",
    "    ax.set_title(metric_name, fontsize=12)\n",
    "    ax.set_ylabel(metric_name.split('(')[0].strip())\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{val:.1f}', ha='center', fontsize=11)\n",
    "\n",
    "fig.suptitle('Whisper Model Size Comparison on Dothraki', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Transcription Analysis\n",
    "\n",
    "Let's examine what Whisper actually outputs when it hears Dothraki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show interesting examples from the small model\n",
    "results = eval_small['results']\n",
    "nonempty = [r for r in results if r.get('whisper_text', '').strip() and 'error' not in r]\n",
    "\n",
    "print(f'Non-empty transcriptions: {len(nonempty)}/{len(results)}')\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('SAMPLE TRANSCRIPTIONS (whisper-small)')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "\n",
    "for r in nonempty[:10]:\n",
    "    print(f'Ground Truth (Dothraki): {r[\"gt_dothraki\"]}')\n",
    "    print(f'Whisper Output ({r[\"whisper_lang\"]}):  {r[\"whisper_text\"][:100]}')\n",
    "    print(f'Ground Truth (English):  {r[\"gt_english\"]}')\n",
    "    print(f'Pipeline Translation:    {r[\"pipeline_translation\"][:100]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper output length vs ground truth length\n",
    "whisper_lens = [len(r.get('whisper_text', '')) for r in results if 'error' not in r]\n",
    "gt_lens = [len(r['gt_dothraki']) for r in results if 'error' not in r]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(gt_lens, whisper_lens, alpha=0.4, c='#4ecdc4', s=30, edgecolors='none')\n",
    "ax.plot([0, max(gt_lens)], [0, max(gt_lens)], 'r--', alpha=0.5, label='y = x (perfect match)')\n",
    "ax.set_xlabel('Ground Truth Length (chars)')\n",
    "ax.set_ylabel('Whisper Output Length (chars)')\n",
    "ax.set_title('Whisper Output Length vs Ground Truth Length')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Correlation between GT and Whisper output lengths: {np.corrcoef(gt_lens, whisper_lens)[0,1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What words does Whisper hallucinate most often?\n",
    "all_whisper_words = []\n",
    "for r in nonempty:\n",
    "    words = r['whisper_text'].lower().split()\n",
    "    all_whisper_words.extend(words)\n",
    "\n",
    "whisper_freq = Counter(all_whisper_words).most_common(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "words, freqs = zip(*whisper_freq)\n",
    "ax.bar(range(len(words)), freqs, color='#ff6b6b', edgecolor='#1a1a2e', alpha=0.8)\n",
    "ax.set_xticks(range(len(words)))\n",
    "ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Most Common Words in Whisper\\'s Output (Hallucinated from Dothraki)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Real vs Synthetic Audio\n",
    "\n",
    "Comparing pipeline behavior on espeak-ng synthesized audio vs real GoT audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real audio results if available\n",
    "real_results_path = RESULTS_DIR / 'drogo_speech_clean_transcription.json'\n",
    "if not real_results_path.exists():\n",
    "    real_results_path = RESULTS_DIR / 'drogo_rhaego_speech_transcription.json'\n",
    "\n",
    "if real_results_path.exists():\n",
    "    real_result = json.loads(real_results_path.read_text())\n",
    "    print(f'Real audio result ({real_results_path.name}):')\n",
    "    print(f'  Detected language: {real_result.get(\"language\", \"N/A\")}')\n",
    "    print(f'  Segments: {len(real_result.get(\"segments\", []))}')\n",
    "    print(f'  Full text preview: {real_result.get(\"text\", \"\")[:200]}...')\n",
    "    \n",
    "    print(f'\\nComparison:')\n",
    "    print(f'  Synthetic clips → Whisper detects English {eval_small[\"stats\"][\"language_distribution\"].get(\"en\", 0)/eval_small[\"stats\"][\"successful\"]*100:.0f}% of the time')\n",
    "    print(f'  Real audio → Whisper detects: {real_result.get(\"language\", \"N/A\")}')\n",
    "    print(f'\\n  Synthetic: clean IPA-synthesized speech, no background noise')\n",
    "    print(f'  Real: actor performance with music, SFX, crowd noise')\n",
    "else:\n",
    "    print('No real audio results found. Run the pipeline on data/raw/ clips first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Key Findings\n",
    "\n",
    "### Language Detection\n",
    "- Whisper overwhelmingly classifies Dothraki as **English** (~85% with whisper-small)\n",
    "- The smaller model (tiny) shows more diverse language guesses including Thai, Korean, and Japanese\n",
    "- This suggests Dothraki's phonology overlaps most with English from Whisper's perspective\n",
    "  (likely due to the actors being English speakers, even when pronouncing Dothraki)\n",
    "\n",
    "### Transcription Quality\n",
    "- ~22-25% of clips produce empty transcriptions (Whisper gives up)\n",
    "- When it does produce text, the output is phonetically influenced but semantically meaningless\n",
    "- Longer Dothraki phrases produce proportionally longer Whisper outputs\n",
    "\n",
    "### Model Size Effect\n",
    "- Larger models are more confident (converge to fewer languages)\n",
    "- Larger models produce more text but not necessarily more useful text\n",
    "- The empty transcription rate is similar across sizes\n",
    "\n",
    "### Implications for Matching\n",
    "- The pipeline must work with phonetically garbled English-like output\n",
    "- Phoneme-level matching (IPA edit distance) is more appropriate than word-level matching\n",
    "- Synthetic audio gives a controlled baseline; real audio adds noise, reverb, and emotional variation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
